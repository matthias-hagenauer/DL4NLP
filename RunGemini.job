#!/bin/bash

#SBATCH --partition=gpu_a100
#SBATCH --gpus=1
#SBATCH --job-name=RunGemini
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=9
#SBATCH --time=12:00:00
#SBATCH --output=output/slurm_output_%A.out

module purge
module load 2024
module load Miniconda3/24.7.1-0

source activate nlp

cd "$HOME/dl4nlp/DL4NLP" || { echo "Bad cd"; exit 1; }

# Load userâ€™s secret token
source ~/.hf_token

# Optional caches (safe to keep in script/repo)
export HF_HOME=/gpfs/home/$USER/.hf
export TRANSFORMERS_CACHE=$HF_HOME/transformers
export HF_HUB_CACHE=$HF_HOME/hub

# Baseline HuggingFace Gemma-3
srun python main.py \
  --model_id G3 \
  --data data/wmt24_filtered_100.jsonl

# # 2-bit quantized GGUF
# srun python main.py \
#   --model_id G3_2bit \
#   --data data/wmt24_filtered_100.jsonl \
#   --n_gpu_layers 40

# # 4-bit quantized GGUF (with explicit local override)
# srun python main.py \
#   --model_id G3_4bit \
#   --gguf_path models/gguf/4bit/gemma-3-12b-it-Q4_K_M.gguf \
#   --data data/wmt24_filtered_100.jsonl \
#   --n_gpu_layers 40

# # 8-bit quantized GGUF
# srun python main.py \
#   --model_id G3_8bit \
#   --data data/wmt24_filtered_100.jsonl \
#   --n_gpu_layers 40
