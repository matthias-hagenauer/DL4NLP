#!/bin/bash

#SBATCH --partition=gpu_mig
#SBATCH --gpus=1
#SBATCH --job-name=RunGemini
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=9
#SBATCH --time=8:00:00
#SBATCH --output=output/slurm_output_%A.out

module purge
module load 2024
module load Miniconda3/24.7.1-0

source activate nlp

cd "$HOME/dl4nlp/DL4NLP" || { echo "Bad cd"; exit 1; }

# Load secrets/env vars
set -a
source .env
set +a

mkdir -p "$HF_HOME" "$HF_HUB_CACHE" "$HF_ASSETS_CACHE"

# Baseline HuggingFace Gemma-3
srun python main.py \
  --model_id G3 \
  --data data/wmt24_filtered_5.jsonl

# # 2-bit quantized GGUF
# srun python main.py \
#   --model_id G3_2bit \
#   --data data/wmt24_filtered_100.jsonl \
#   --n_gpu_layers 40

# # 4-bit quantized GGUF (with explicit local override)
# srun python main.py \
#   --model_id G3_4bit \
#   --gguf_path models/gguf/4bit/gemma-3-12b-it-Q4_K_M.gguf \
#   --data data/wmt24_filtered_100.jsonl \
#   --n_gpu_layers 40

# # 8-bit quantized GGUF
# srun python main.py \
#   --model_id G3_8bit \
#   --data data/wmt24_filtered_100.jsonl \
#   --n_gpu_layers 40
