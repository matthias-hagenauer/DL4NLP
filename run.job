#!/bin/bash

#SBATCH --partition=gpu_a100
#SBATCH --gpus=1
#SBATCH --job-name=Run
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=9
#SBATCH --time=04:00:00
#SBATCH --output=output/slurm_output_%A.out

module purge
module load 2024
module load Miniconda3/24.7.1-0

source activate nlp

cd $HOME/dl4nlp/DL4NLP/

# Baseline HuggingFace TowerMistral
srun python main.py --model_id TM --data data/wmt24_filtered_5.jsonl --bins quantile_balanced

# # 2-bit quantized GGUF (local if available, else HF)
# srun python main.py --model_id TM_2bit --data data/wmt24_filtered_5.jsonl --n_gpu_layers 40 --bins quantile_balanced

# # 3-bit quantized GGUF
# srun python main.py --model_id TM_3bit --data data/wmt24_filtered_5.jsonl --n_gpu_layers 40 --bins quantile_balanced

# # 4-bit quantized GGUF (with explicit local override)
# srun python main.py --model_id TM_4bit --gguf_path models/gguf/4bit/TowerInstruct-Mistral-7B-v0.2-Q4_K_M.gguf --data data/balanced_100.jsonl --n_gpu_layers 40 --bins quantile_balanced

# # 5-bit quantized GGUF
# srun python main.py --model_id TM_5bit --data data/wmt24_filtered_5.jsonl --n_gpu_layers 40 --bins quantile_balanced

# # 6-bit quantized GGUF
# srun python main.py --model_id TM_6bit --data data/wmt24_filtered_5.jsonl --n_gpu_layers 40 --bins quantile_balanced

# # 8-bit quantized GGUF
# srun python main.py --model_id TM_8bit --data data/balanced_100.jsonl --n_gpu_layers 40 --bins quantile_balanced
