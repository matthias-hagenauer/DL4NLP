#!/bin/bash

#SBATCH --partition=gpu_a100
#SBATCH --gpus=1
#SBATCH --job-name=Run
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=9
#SBATCH --time=12:00:00
#SBATCH --output=output/slurm_output_%A.out

module purge
module load 2024
module load Miniconda3/24.7.1-0

source activate nlp

cd "$HOME/dl4nlp/DL4NLP" || { echo "Bad cd"; exit 1; }


# Baseline HuggingFace TowerMistral
# srun python main.py --model_id TM --data data/wmt24_estimated_normalized.jsonl --bins quantile_balanced --eval_metrics chrf bleu comet

# # 2-bit quantized GGUF (local if available, else HF)
# srun python main.py --model_id TM_2bit --data data/wmt24_estimated_normalized.jsonl --n_gpu_layers 40 --bins quantile_balanced --eval_metrics chrf bleu comet

# # 3-bit quantized GGUF
# srun python main.py --model_id TM_3bit --data data/wmt24_estimated_normalized.jsonl --n_gpu_layers 40 --bins quantile_balanced --eval_metrics chrf bleu comet

# # 4-bit quantized GGUF (with explicit local override)
# srun python main.py --model_id TM_4bit --data data/wmt24_estimated_normalized.jsonl --n_gpu_layers 40 --bins quantile_balanced --eval_metrics chrf bleu comet

# # 5-bit quantized GGUF
# srun python main.py --model_id TM_5bit --data data/wmt24_estimated_normalized.jsonl --n_gpu_layers 40 --bins quantile_balanced --eval_metrics chrf bleu comet

# # 6-bit quantized GGUF
# srun python main.py --model_id TM_6bit --data data/wmt24_estimated_normalized.jsonl --n_gpu_layers 40 --bins quantile_balanced --eval_metrics chrf bleu comet

# # 8-bit quantized GGUF
# srun python main.py --model_id TM_8bit --data data/wmt24_estimated_normalized.jsonl --n_gpu_layers 40 --bins quantile_balanced --eval_metrics chrf bleu comet

# python analyze_bin.py results/
python plot_in_bin_statistics.py
